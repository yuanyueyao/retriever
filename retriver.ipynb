{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-21 08:38:56.430\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mtext2vec.sentence_model\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m78\u001b[0m - \u001b[34m\u001b[1mUse pytorch device: cuda\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized FAISS index of type <class 'faiss.swigfaiss.IndexFlatIP'>\n",
      "Loading Excel file...\n",
      "Loaded Excel file: term_work/sampled-10k.xlsx, total rows: 10000\n",
      "Loading embeddings from term_work/embeddings-10k.npy...\n",
      "Loaded embeddings from file, shape: (10000, 768)\n",
      "Total data indexed: 10000\n",
      "Indexing done!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4'\n",
    "\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import normalize\n",
    "from text2vec import SentenceModel\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "class ExcelIndexer(object):\n",
    "    def __init__(self, vector_sz: int, n_subquantizers=0, n_bits=8, model: SentenceModel = None, embeddings_file: str = None):\n",
    "        \"\"\"初始化索引器，选择使用FAISS的类型\"\"\"\n",
    "        if n_subquantizers > 0:\n",
    "            self.index = faiss.IndexPQ(vector_sz, n_subquantizers, n_bits, faiss.METRIC_INNER_PRODUCT)\n",
    "        else:\n",
    "            self.index = faiss.IndexFlatIP(vector_sz)\n",
    "        self.index_id_to_row_id = []  # 用于存储 FAISS 索引 ID 到 Excel 文件中的行号映射\n",
    "\n",
    "        self.data_frame = None  # 存储 Excel 数据\n",
    "        self.model = model\n",
    "        self.embeddings_file = embeddings_file  # 存储嵌入向量文件路径\n",
    "\n",
    "        print(f'Initialized FAISS index of type {type(self.index)}')\n",
    "\n",
    "    def load_excel(self, excel_file: str, id_column: str, batch_size: int = 2048):\n",
    "        \"\"\"\n",
    "        加载 Excel 文件并自动创建 FAISS 索引\n",
    "        :param excel_file: Excel 文件路径\n",
    "        :param vector_column: 存储嵌入向量的列名\n",
    "        :param id_column: 存储唯一标识符的列名\n",
    "        \"\"\"\n",
    "        # 加载 Excel 文件到 DataFrame\n",
    "        print('Loading Excel file...')\n",
    "        self.data_frame = pd.read_excel(excel_file)\n",
    "        print(f'Loaded Excel file: {excel_file}, total rows: {len(self.data_frame)}')\n",
    "        lenth = len(self.data_frame)\n",
    "\n",
    "        # 如果存在保存的嵌入向量文件，直接加载它\n",
    "        if self.embeddings_file and os.path.exists(self.embeddings_file):\n",
    "            print(f'Loading embeddings from {self.embeddings_file}...')\n",
    "            self.embeddings = np.load(self.embeddings_file)\n",
    "            print(f'Loaded embeddings from file, shape: {self.embeddings.shape}')\n",
    "            ids = range(lenth)\n",
    "            self.index_data(ids, self.embeddings)\n",
    "        else:\n",
    "            self.embeddings = np.empty((lenth, self.index.d), dtype=np.float32)  # 初始化一个空的嵌入向量矩阵\n",
    "            # 提取 ID 和向量数据\n",
    "            print('Indexing data...')\n",
    "            for times in range(lenth // batch_size + 1):\n",
    "                print(f'Indexing batch {times + 1}/{lenth // batch_size + 1}, total indexed: {len(self.index_id_to_row_id)}, total data: {lenth}')\n",
    "                start = times * batch_size\n",
    "                end = min((times + 1) * batch_size, lenth)\n",
    "                ids = range(start, end)\n",
    "                embeddings = np.array([self.model.encode(self.data_frame[id_column][i]) for i in range(start, end)]).astype('float32')\n",
    "\n",
    "                # 保存嵌入向量到内存中\n",
    "                self.embeddings[start:end] = embeddings\n",
    "                self.index_data(ids, embeddings)\n",
    "\n",
    "            # 保存嵌入向量到文件\n",
    "            if self.embeddings_file:\n",
    "                print(f'Saving embeddings to {self.embeddings_file}...')\n",
    "                np.save(self.embeddings_file, self.embeddings)\n",
    "\n",
    "        print('Indexing done!')\n",
    "\n",
    "    def index_data(self, ids: List[int], embeddings: np.array):\n",
    "        \"\"\"\n",
    "        将数据从 Excel 中加载并索引\n",
    "        :param ids: 来自 Excel 的行 ID（可以是某一列唯一标识符）\n",
    "        :param embeddings: 行的嵌入向量\n",
    "        \"\"\"\n",
    "        # 更新 ID 映射\n",
    "        self._update_id_mapping(ids)\n",
    "\n",
    "        # 将 embeddings 转换为 float32 类型\n",
    "        embeddings = embeddings.astype('float32')\n",
    "\n",
    "        # 如果 FAISS 索引尚未训练，则进行训练\n",
    "        if not self.index.is_trained:\n",
    "            self.index.train(embeddings)\n",
    "\n",
    "        # 将 embeddings 添加到 FAISS 索引\n",
    "        self.index.add(embeddings)\n",
    "        print(f'Total data indexed: {len(self.index_id_to_row_id)}')\n",
    "\n",
    "    # 其他方法保持不变\n",
    "\n",
    "    def _update_id_mapping(self, row_ids: List[int]):\n",
    "        \"\"\"更新行 ID 到索引 ID 的映射关系\"\"\"\n",
    "        self.index_id_to_row_id.extend(row_ids)\n",
    "\n",
    "    def search_return_text(self, query: str, top_docs: int, index_batch_size: int = 10000) -> List[Tuple[List[object], List[float]]]:\n",
    "        search_result = self.search_dp(query, top_docs, index_batch_size)\n",
    "        result = []\n",
    "        for i in search_result:\n",
    "            result.append(([self.data_frame['描述'][int(j)] for j in i[0] if len(self.data_frame['描述'][j]) > 10 ],i[1]))  #过滤掉描述长度小于10的\n",
    "        return result\n",
    "    def search_dp(self, query: str, top_docs: int, index_batch_size: int = 2048) -> List[Tuple[List[object], List[float]]]:\n",
    "        \"\"\"\n",
    "        执行 dp 查询，返回 Excel 文件行 ID 和相似度得分\n",
    "        :param query_vectors: 查询的嵌入向量\n",
    "        :param top_docs: 返回的最近邻文档数量\n",
    "        :param index_batch_size: 每批次处理的查询数量\n",
    "        :return: 返回每个查询向量对应的最近邻行 ID 和得分\n",
    "        \"\"\"\n",
    "        query_vectors = self.model.encode([query]).astype('float32')\n",
    "        result = []  # 存储所有查询结果\n",
    "\n",
    "        # 计算批次数量\n",
    "        nbatch = (len(query_vectors) - 1) // index_batch_size + 1\n",
    "\n",
    "        # 批量处理查询\n",
    "        for k in tqdm(range(nbatch)):\n",
    "            start_idx = k * index_batch_size\n",
    "            end_idx = min((k + 1) * index_batch_size, len(query_vectors))\n",
    "\n",
    "            q = query_vectors[start_idx: end_idx]\n",
    "\n",
    "            # 使用 FAISS 进行搜索\n",
    "            scores, indexes = self.index.search(q, top_docs)\n",
    "\n",
    "            # 将 FAISS 索引 ID 转换为 Excel 中的行 ID\n",
    "            db_ids = [[str(self.index_id_to_row_id[i]) for i in query_top_idxs] for query_top_idxs in indexes]\n",
    "\n",
    "            # 将每个查询结果添加到最终结果中\n",
    "            result.extend([(db_ids[i], scores[i]) for i in range(len(db_ids))])\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def search_return_text(self, query: str, top_docs: int, index_batch_size: int = 10000) -> List[Tuple[List[object], List[float]]]:\n",
    "        search_result = self.search_dp(query, top_docs, index_batch_size)\n",
    "        result = []\n",
    "        for i in search_result:\n",
    "            result.append(([self.data_frame['描述'][int(j)] for j in i[0]],i[1]))\n",
    "        return result\n",
    "    \n",
    "\n",
    "# model =  SentenceModel('shibing624/text2vec-base-chinese')\n",
    "# retriever = ExcelIndexer(vector_sz=768, model=model, embeddings_file='term_work/embeddings.npy')\n",
    "# retriever.load_excel('term_work/更新后的网格文件_提取矛盾类型和严重程度_提取区名.xlsx','描述')\n",
    "\n",
    "\n",
    "model =  SentenceModel('BAAI/bge-large-zh-v1.5')\n",
    "retriever = ExcelIndexer(vector_sz=1024, model=model, embeddings_file='term_work/embeddings-10k.npy')\n",
    "retriever.load_excel('term_work/sampled-10k.xlsx','描述')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 198.98it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['6646', '8723', '6891', '9249', '5272'],\n",
       " array([132.77771, 130.60011, 129.99629, 128.727  , 126.65196],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.search_dp('我吃柠檬', 5, 10000)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'来电人反映：2月初老人到历下区坤顺路劳动监察部门反映公司的问题，受理的工作人员（杨先生，年纪不大）态度很差，一直让老人跑手续，要求投诉工作人员。希望相关单位落实处理，请处理。'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retriever.data_frame['描述'][123])\n",
    "retriever.data_frame['描述'][123]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 190.30it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(['王先生来电反映：天桥区北园银座超市购买蓝莓，标价写的是每500克，12.8元，但是结账时显示每盒12.8元，认为不合理，来电投诉，要求查处治理。希望相关单位落实处理，请处理并回复来电人。',\n",
       "   '吴女士来电反映：每天17:00左右，天桥区经一纬五路口有卖水果的车辆占道经营，要求查处。（不能提供所属街道）希望相关单位落实处理，请处理并回复来电人。',\n",
       "   '网友微信反映：槐荫区经六路桃园北区13号楼单元门口污水井外溢，臭气熏天，希望相关单位落实处理，请处理。',\n",
       "   '网友“苏海烨”微信反映：2023年5月5日，在被投诉举报人大润发超市莱芜店（济南市莱芜区花园北路与汶源东大街交界处）购买天天想爽口榨菜，单价3.9元，标注产品类型，：酱腌菜（盐渍菜），产品标准代号：SB／T10439，配料：榨菜、食用盐、辣椒、水、香辛料、食品添加剂（苯甲酸钠、山梨酸钾、柠檬黄、焦亚硫酸钠）。产品配料中添加有水，固形物不低于80%，SB／T10439为酱腌菜标准，该标准第3.2条盐渍菜（以蔬菜为原料，用食盐，盐渍加工而成的蔬菜制品），3.8条盐水渍菜（以蔬菜为原料，用盐水经生渍或熟渍加工而成的蔬菜制品），该产品的类型应为盐水渍菜，该产品虚假标注产品类型。综上，大润发超市销售不符合食品安全标准的食品。1请求依法查处，给予行政处罚，2处理完毕，按国家规定给予奖励，3责令销售企业按国家规定召回问题产品，4处理结果书面回复投诉举报人。希望相关单位落实处理，请处理并回复诉求人。',\n",
       "   '家里厕所水管漏水，影响正常使用，请物业上门解决。'],\n",
       "  array([132.77771, 130.60011, 129.99629, 128.727  , 126.65196],\n",
       "        dtype=float32))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.search_return_text('我吃柠檬', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-21 08:42:26.129\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mtext2vec.sentence_model\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m78\u001b[0m - \u001b[34m\u001b[1mUse pytorch device: cuda\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.28829545 -0.5994263   0.10103194 ...  0.19538179  0.7825346\n",
      "  -0.609733  ]\n",
      " [ 0.7193505  -0.4386894  -0.3302605  ...  0.33385447  0.19717875\n",
      "  -0.7886755 ]\n",
      " [ 0.5331809   0.12236019  0.46085203 ... -0.7543797   0.08203759\n",
      "   0.24746926]\n",
      " [ 0.24122617  0.3629665   0.59680206 ... -0.70514524  0.51105595\n",
      "  -0.39896932]]\n"
     ]
    }
   ],
   "source": [
    "from text2vec import SentenceModel\n",
    "sentences = ['如何更换花呗绑定银行卡', '花呗更改绑定银行卡','火影忍者真好看','火影忍者作者画的挺好的']\n",
    "\n",
    "model = SentenceModel('BAAI/bge-large-zh-v1.5')\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "292.21848"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[2]@embeddings[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 3 2]]\n",
      "如何更换花呗绑定银行卡\n",
      "花呗更改绑定银行卡\n",
      "火影忍者作者画的挺好的\n",
      "火影忍者真好看\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "# 假设你已经有多个句子的嵌入向量\n",
    "embeddings = model.encode(sentences)  # 生成多个句子的嵌入\n",
    "embeddings = F.normalize(torch.tensor(embeddings), p=2, dim=1).numpy()\n",
    "# 将向量转为 numpy 数组\n",
    "embeddings_array = np.array(embeddings).astype('float32')\n",
    "# 创建 FAISS 索引\n",
    "index = faiss.IndexFlatL2(embeddings_array.shape[1])\n",
    "index.add(embeddings_array)\n",
    "\n",
    "# 查询相似向量\n",
    "query_embedding = model.encode([\"查询的句子\"])[0].reshape(1, -1)\n",
    "distances, indices = index.search(query_embedding,k=4)  # k 为返回的相似向量数量\n",
    "print(indices)  # 输出相似句子的索引\n",
    "# 输出相似句子\n",
    "for index in indices[0]:\n",
    "    print(sentences[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-21 08:41:34.059\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mtext2vec.sentence_model\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m78\u001b[0m - \u001b[34m\u001b[1mUse pytorch device: cuda\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import normalize\n",
    "class ExcelIndexer(object):\n",
    "    def __init__(self, vector_sz: int, n_subquantizers=0, n_bits=8, model = SentenceModel('BAAI/bge-large-zh-v1.5')):\n",
    "        \"\"\"初始化索引器，选择使用FAISS的类型\"\"\"\n",
    "        if n_subquantizers > 0:\n",
    "            self.index = faiss.IndexPQ(vector_sz, n_subquantizers, n_bits, faiss.METRIC_INNER_PRODUCT)\n",
    "        else:\n",
    "            self.index = faiss.IndexFlatIP(vector_sz)\n",
    "        self.index_id_to_row_id = []  # 用于存储 FAISS 索引 ID 到 Excel 文件中的行号映射\n",
    "\n",
    "        self.data_frame = None  # 存储 Excel 数据\n",
    "        self.model = model\n",
    "        \n",
    "    \n",
    "\n",
    "    def load_excel(self, excel_file: str, id_column: str, batch_size: int = 20000):\n",
    "        \"\"\"\n",
    "        加载 Excel 文件并自动创建 FAISS 索引\n",
    "        :param excel_file: Excel 文件路径\n",
    "        :param vector_column: 存储嵌入向量的列名\n",
    "        :param id_column: 存储唯一标识符的列名\n",
    "        \"\"\"\n",
    "        # 加载 Excel 文件到 DataFrame\n",
    "        print('Loading Excel file...')\n",
    "        self.data_frame = pd.read_excel(excel_file)\n",
    "        print(f'Loaded Excel file: {excel_file}, total rows: {len(self.data_frame)}')\n",
    "        lenth = len(self.data_frame)\n",
    "        # 提取 ID 和向量数据\n",
    "        print('Indexing data...')\n",
    "        for times in range(lenth//batch_size+1):\n",
    "            print(f'Indexing batch {times+1}/{lenth//batch_size+1}, total indexed: {len(self.index_id_to_row_id)}, total data: {lenth}')\n",
    "            start = times*batch_size\n",
    "            end = min((times+1)*batch_size,lenth)\n",
    "            ids = range(start,end)\n",
    "            embeddings = np.array([model.encode(self.data_frame[id_column][i]) for i in range(start,end)]).astype('float32')\n",
    "            self.index_data(ids, embeddings)\n",
    "        print('Indexing done!')\n",
    "        # 根据id_column索引数据\n",
    "        \n",
    "    def index_data(self, ids: List[int], embeddings: np.array):\n",
    "        \"\"\"\n",
    "        将数据从 Excel 中加载并索引\n",
    "        :param ids: 来自 Excel 的行 ID（可以是某一列唯一标识符）\n",
    "        :param embeddings: 行的嵌入向量\n",
    "        \"\"\"\n",
    "        # 更新 ID 映射\n",
    "        self._update_id_mapping(ids)\n",
    "\n",
    "        # 将 embeddings 转换为 float32 类型\n",
    "        embeddings = embeddings.astype('float32')\n",
    "\n",
    "        # 如果 FAISS 索引尚未训练，则进行训练\n",
    "        if not self.index.is_trained:\n",
    "            self.index.train(embeddings)\n",
    "\n",
    "        # 将 embeddings 添加到 FAISS 索引\n",
    "        self.index.add(embeddings)\n",
    "        print(f'Total data indexed: {len(self.index_id_to_row_id)}')\n",
    "        \n",
    "\n",
    "    def search_dp(self, query: str, top_docs: int, index_batch_size: int = 2048) -> List[Tuple[List[object], List[float]]]:\n",
    "        \"\"\"\n",
    "        执行 dp 查询，返回 Excel 文件行 ID 和相似度得分\n",
    "        :param query_vectors: 查询的嵌入向量\n",
    "        :param top_docs: 返回的最近邻文档数量\n",
    "        :param index_batch_size: 每批次处理的查询数量\n",
    "        :return: 返回每个查询向量对应的最近邻行 ID 和得分\n",
    "        \"\"\"\n",
    "        query_vectors = model.encode([query]).astype('float32')\n",
    "        result = []  # 存储所有查询结果\n",
    "\n",
    "        # 计算批次数量\n",
    "        nbatch = (len(query_vectors) - 1) // index_batch_size + 1\n",
    "\n",
    "        # 批量处理查询\n",
    "        for k in tqdm(range(nbatch)):\n",
    "            start_idx = k * index_batch_size\n",
    "            end_idx = min((k + 1) * index_batch_size, len(query_vectors))\n",
    "\n",
    "            q = query_vectors[start_idx: end_idx]\n",
    "\n",
    "            # 使用 FAISS 进行搜索\n",
    "            scores, indexes = self.index.search(q, top_docs)\n",
    "\n",
    "            # 将 FAISS 索引 ID 转换为 Excel 中的行 ID\n",
    "            db_ids = [[str(self.index_id_to_row_id[i]) for i in query_top_idxs] for query_top_idxs in indexes]\n",
    "\n",
    "            # 将每个查询结果添加到最终结果中\n",
    "            result.extend([(db_ids[i], scores[i]) for i in range(len(db_ids))])\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def search_return_text(self, query: str, top_docs: int, index_batch_size: int = 10000) -> List[Tuple[List[object], List[float]]]:\n",
    "        search_result = self.search_dp(query, top_docs, index_batch_size)\n",
    "        result = []\n",
    "        for i in search_result:\n",
    "            result.append(([self.data_frame['描述'][int(j)] for j in i[0]],i[1]))\n",
    "        return result\n",
    "    \n",
    "\n",
    "    def serialize(self, dir_path: str):\n",
    "        \"\"\"\n",
    "        将索引和映射关系序列化到磁盘\n",
    "        :param dir_path: 保存的路径\n",
    "        \"\"\"\n",
    "        index_file = os.path.join(dir_path, 'index.faiss')\n",
    "        meta_file = os.path.join(dir_path, 'index_meta.pkl')\n",
    "\n",
    "        print(f'Serializing index to {index_file}, meta data to {meta_file}')\n",
    "\n",
    "        faiss.write_index(self.index, index_file)\n",
    "        with open(meta_file, mode='wb') as f:\n",
    "            pickle.dump(self.index_id_to_row_id, f)\n",
    "\n",
    "    def deserialize_from(self, dir_path: str):\n",
    "        \"\"\"\n",
    "        从磁盘加载索引和映射关系\n",
    "        :param dir_path: 文件存储路径\n",
    "        \"\"\"\n",
    "        index_file = os.path.join(dir_path, 'index.faiss')\n",
    "        meta_file = os.path.join(dir_path, 'index_meta.pkl')\n",
    "\n",
    "        print(f'Loading index from {index_file}, meta data from {meta_file}')\n",
    "\n",
    "        self.index = faiss.read_index(index_file)\n",
    "        print(f'Loaded index of type {type(self.index)} and size {self.index.ntotal}')\n",
    "\n",
    "        with open(meta_file, \"rb\") as f:\n",
    "            self.index_id_to_row_id = pickle.load(f)\n",
    "\n",
    "        assert len(self.index_id_to_row_id) == self.index.ntotal, 'Deserialized index_id_to_row_id should match FAISS index size'\n",
    "\n",
    "    def _update_id_mapping(self, row_ids: List[int]):\n",
    "        \"\"\"更新行 ID 到索引 ID 的映射关系\"\"\"\n",
    "        self.index_id_to_row_id.extend(row_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Excel file...\n",
      "Loaded Excel file: term_work/sampled-10k.xlsx, total rows: 10000\n",
      "Indexing data...\n",
      "Indexing batch 1/1, total indexed: 0, total data: 10000\n",
      "Total data indexed: 10000\n",
      "Indexing done!\n"
     ]
    }
   ],
   "source": [
    "retriever = ExcelIndexer(vector_sz=1024)\n",
    "retriever.load_excel('term_work/sampled-10k.xlsx','描述')\n",
    "# retriever.load_excel('retriever_test.xlsx','描述')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 149,\n",
       " 150,\n",
       " 151,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 157,\n",
       " 158,\n",
       " 159,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 165,\n",
       " 166,\n",
       " 167,\n",
       " 168,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 172,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 180,\n",
       " 181,\n",
       " 182,\n",
       " 183,\n",
       " 184,\n",
       " 185,\n",
       " 186,\n",
       " 187,\n",
       " 188,\n",
       " 189,\n",
       " 190,\n",
       " 191,\n",
       " 192,\n",
       " 193,\n",
       " 194,\n",
       " 195,\n",
       " 196,\n",
       " 197,\n",
       " 198,\n",
       " 199,\n",
       " 200,\n",
       " 201,\n",
       " 202,\n",
       " 203,\n",
       " 204,\n",
       " 205,\n",
       " 206,\n",
       " 207,\n",
       " 208,\n",
       " 209,\n",
       " 210,\n",
       " 211,\n",
       " 212,\n",
       " 213,\n",
       " 214,\n",
       " 215,\n",
       " 216,\n",
       " 217,\n",
       " 218,\n",
       " 219,\n",
       " 220,\n",
       " 221,\n",
       " 222,\n",
       " 223,\n",
       " 224,\n",
       " 225,\n",
       " 226,\n",
       " 227,\n",
       " 228,\n",
       " 229,\n",
       " 230,\n",
       " 231,\n",
       " 232,\n",
       " 233,\n",
       " 234,\n",
       " 235,\n",
       " 236,\n",
       " 237,\n",
       " 238,\n",
       " 239,\n",
       " 240,\n",
       " 241,\n",
       " 242,\n",
       " 243,\n",
       " 244,\n",
       " 245,\n",
       " 246,\n",
       " 247,\n",
       " 248,\n",
       " 249,\n",
       " 250,\n",
       " 251,\n",
       " 252,\n",
       " 253,\n",
       " 254,\n",
       " 255,\n",
       " 256,\n",
       " 257,\n",
       " 258,\n",
       " 259,\n",
       " 260,\n",
       " 261,\n",
       " 262,\n",
       " 263,\n",
       " 264,\n",
       " 265,\n",
       " 266,\n",
       " 267,\n",
       " 268,\n",
       " 269,\n",
       " 270,\n",
       " 271,\n",
       " 272,\n",
       " 273,\n",
       " 274,\n",
       " 275,\n",
       " 276,\n",
       " 277,\n",
       " 278,\n",
       " 279,\n",
       " 280,\n",
       " 281,\n",
       " 282,\n",
       " 283,\n",
       " 284,\n",
       " 285,\n",
       " 286,\n",
       " 287,\n",
       " 288,\n",
       " 289,\n",
       " 290,\n",
       " 291,\n",
       " 292,\n",
       " 293,\n",
       " 294,\n",
       " 295,\n",
       " 296,\n",
       " 297,\n",
       " 298,\n",
       " 299,\n",
       " 300,\n",
       " 301,\n",
       " 302,\n",
       " 303,\n",
       " 304,\n",
       " 305,\n",
       " 306,\n",
       " 307,\n",
       " 308,\n",
       " 309,\n",
       " 310,\n",
       " 311,\n",
       " 312,\n",
       " 313,\n",
       " 314,\n",
       " 315,\n",
       " 316,\n",
       " 317,\n",
       " 318,\n",
       " 319,\n",
       " 320,\n",
       " 321,\n",
       " 322,\n",
       " 323,\n",
       " 324,\n",
       " 325,\n",
       " 326,\n",
       " 327,\n",
       " 328,\n",
       " 329,\n",
       " 330,\n",
       " 331,\n",
       " 332,\n",
       " 333,\n",
       " 334,\n",
       " 335,\n",
       " 336,\n",
       " 337,\n",
       " 338,\n",
       " 339,\n",
       " 340,\n",
       " 341,\n",
       " 342,\n",
       " 343,\n",
       " 344,\n",
       " 345,\n",
       " 346,\n",
       " 347,\n",
       " 348,\n",
       " 349,\n",
       " 350,\n",
       " 351,\n",
       " 352,\n",
       " 353,\n",
       " 354,\n",
       " 355,\n",
       " 356,\n",
       " 357,\n",
       " 358,\n",
       " 359,\n",
       " 360,\n",
       " 361,\n",
       " 362,\n",
       " 363,\n",
       " 364,\n",
       " 365,\n",
       " 366,\n",
       " 367,\n",
       " 368,\n",
       " 369,\n",
       " 370,\n",
       " 371,\n",
       " 372,\n",
       " 373,\n",
       " 374,\n",
       " 375,\n",
       " 376,\n",
       " 377,\n",
       " 378,\n",
       " 379,\n",
       " 380,\n",
       " 381,\n",
       " 382,\n",
       " 383,\n",
       " 384,\n",
       " 385,\n",
       " 386,\n",
       " 387,\n",
       " 388,\n",
       " 389,\n",
       " 390,\n",
       " 391,\n",
       " 392,\n",
       " 393,\n",
       " 394,\n",
       " 395,\n",
       " 396,\n",
       " 397,\n",
       " 398,\n",
       " 399,\n",
       " 400,\n",
       " 401,\n",
       " 402,\n",
       " 403,\n",
       " 404,\n",
       " 405,\n",
       " 406,\n",
       " 407,\n",
       " 408,\n",
       " 409,\n",
       " 410,\n",
       " 411,\n",
       " 412,\n",
       " 413,\n",
       " 414,\n",
       " 415,\n",
       " 416,\n",
       " 417,\n",
       " 418,\n",
       " 419,\n",
       " 420,\n",
       " 421,\n",
       " 422,\n",
       " 423,\n",
       " 424,\n",
       " 425,\n",
       " 426,\n",
       " 427,\n",
       " 428,\n",
       " 429,\n",
       " 430,\n",
       " 431,\n",
       " 432,\n",
       " 433,\n",
       " 434,\n",
       " 435,\n",
       " 436,\n",
       " 437,\n",
       " 438,\n",
       " 439,\n",
       " 440,\n",
       " 441,\n",
       " 442,\n",
       " 443,\n",
       " 444,\n",
       " 445,\n",
       " 446,\n",
       " 447,\n",
       " 448,\n",
       " 449,\n",
       " 450,\n",
       " 451,\n",
       " 452,\n",
       " 453,\n",
       " 454,\n",
       " 455,\n",
       " 456,\n",
       " 457,\n",
       " 458,\n",
       " 459,\n",
       " 460,\n",
       " 461,\n",
       " 462,\n",
       " 463,\n",
       " 464,\n",
       " 465,\n",
       " 466,\n",
       " 467,\n",
       " 468,\n",
       " 469,\n",
       " 470,\n",
       " 471,\n",
       " 472,\n",
       " 473,\n",
       " 474,\n",
       " 475,\n",
       " 476,\n",
       " 477,\n",
       " 478,\n",
       " 479,\n",
       " 480,\n",
       " 481,\n",
       " 482,\n",
       " 483,\n",
       " 484,\n",
       " 485,\n",
       " 486,\n",
       " 487,\n",
       " 488,\n",
       " 489,\n",
       " 490,\n",
       " 491,\n",
       " 492,\n",
       " 493,\n",
       " 494,\n",
       " 495,\n",
       " 496,\n",
       " 497,\n",
       " 498,\n",
       " 499,\n",
       " 500,\n",
       " 501,\n",
       " 502,\n",
       " 503,\n",
       " 504,\n",
       " 505,\n",
       " 506,\n",
       " 507,\n",
       " 508,\n",
       " 509,\n",
       " 510,\n",
       " 511,\n",
       " 512,\n",
       " 513,\n",
       " 514,\n",
       " 515,\n",
       " 516,\n",
       " 517,\n",
       " 518,\n",
       " 519,\n",
       " 520,\n",
       " 521,\n",
       " 522,\n",
       " 523,\n",
       " 524,\n",
       " 525,\n",
       " 526,\n",
       " 527,\n",
       " 528,\n",
       " 529,\n",
       " 530,\n",
       " 531,\n",
       " 532,\n",
       " 533,\n",
       " 534,\n",
       " 535,\n",
       " 536,\n",
       " 537,\n",
       " 538,\n",
       " 539,\n",
       " 540,\n",
       " 541,\n",
       " 542,\n",
       " 543,\n",
       " 544,\n",
       " 545,\n",
       " 546,\n",
       " 547,\n",
       " 548,\n",
       " 549,\n",
       " 550,\n",
       " 551,\n",
       " 552,\n",
       " 553,\n",
       " 554,\n",
       " 555,\n",
       " 556,\n",
       " 557,\n",
       " 558,\n",
       " 559,\n",
       " 560,\n",
       " 561,\n",
       " 562,\n",
       " 563,\n",
       " 564,\n",
       " 565,\n",
       " 566,\n",
       " 567,\n",
       " 568,\n",
       " 569,\n",
       " 570,\n",
       " 571,\n",
       " 572,\n",
       " 573,\n",
       " 574,\n",
       " 575,\n",
       " 576,\n",
       " 577,\n",
       " 578,\n",
       " 579,\n",
       " 580,\n",
       " 581,\n",
       " 582,\n",
       " 583,\n",
       " 584,\n",
       " 585,\n",
       " 586,\n",
       " 587,\n",
       " 588,\n",
       " 589,\n",
       " 590,\n",
       " 591,\n",
       " 592,\n",
       " 593,\n",
       " 594,\n",
       " 595,\n",
       " 596,\n",
       " 597,\n",
       " 598,\n",
       " 599,\n",
       " 600,\n",
       " 601,\n",
       " 602,\n",
       " 603,\n",
       " 604,\n",
       " 605,\n",
       " 606,\n",
       " 607,\n",
       " 608,\n",
       " 609,\n",
       " 610,\n",
       " 611,\n",
       " 612,\n",
       " 613,\n",
       " 614,\n",
       " 615,\n",
       " 616,\n",
       " 617,\n",
       " 618,\n",
       " 619,\n",
       " 620,\n",
       " 621,\n",
       " 622,\n",
       " 623,\n",
       " 624,\n",
       " 625,\n",
       " 626,\n",
       " 627,\n",
       " 628,\n",
       " 629,\n",
       " 630,\n",
       " 631,\n",
       " 632,\n",
       " 633,\n",
       " 634,\n",
       " 635,\n",
       " 636,\n",
       " 637,\n",
       " 638,\n",
       " 639,\n",
       " 640,\n",
       " 641,\n",
       " 642,\n",
       " 643,\n",
       " 644,\n",
       " 645,\n",
       " 646,\n",
       " 647,\n",
       " 648,\n",
       " 649,\n",
       " 650,\n",
       " 651,\n",
       " 652,\n",
       " 653,\n",
       " 654,\n",
       " 655,\n",
       " 656,\n",
       " 657,\n",
       " 658,\n",
       " 659,\n",
       " 660,\n",
       " 661,\n",
       " 662,\n",
       " 663,\n",
       " 664,\n",
       " 665,\n",
       " 666,\n",
       " 667,\n",
       " 668,\n",
       " 669,\n",
       " 670,\n",
       " 671,\n",
       " 672,\n",
       " 673,\n",
       " 674,\n",
       " 675,\n",
       " 676,\n",
       " 677,\n",
       " 678,\n",
       " 679,\n",
       " 680,\n",
       " 681,\n",
       " 682,\n",
       " 683,\n",
       " 684,\n",
       " 685,\n",
       " 686,\n",
       " 687,\n",
       " 688,\n",
       " 689,\n",
       " 690,\n",
       " 691,\n",
       " 692,\n",
       " 693,\n",
       " 694,\n",
       " 695,\n",
       " 696,\n",
       " 697,\n",
       " 698,\n",
       " 699,\n",
       " 700,\n",
       " 701,\n",
       " 702,\n",
       " 703,\n",
       " 704,\n",
       " 705,\n",
       " 706,\n",
       " 707,\n",
       " 708,\n",
       " 709,\n",
       " 710,\n",
       " 711,\n",
       " 712,\n",
       " 713,\n",
       " 714,\n",
       " 715,\n",
       " 716,\n",
       " 717,\n",
       " 718,\n",
       " 719,\n",
       " 720,\n",
       " 721,\n",
       " 722,\n",
       " 723,\n",
       " 724,\n",
       " 725,\n",
       " 726,\n",
       " 727,\n",
       " 728,\n",
       " 729,\n",
       " 730,\n",
       " 731,\n",
       " 732,\n",
       " 733,\n",
       " 734,\n",
       " 735,\n",
       " 736,\n",
       " 737,\n",
       " 738,\n",
       " 739,\n",
       " 740,\n",
       " 741,\n",
       " 742,\n",
       " 743,\n",
       " 744,\n",
       " 745,\n",
       " 746,\n",
       " 747,\n",
       " 748,\n",
       " 749,\n",
       " 750,\n",
       " 751,\n",
       " 752,\n",
       " 753,\n",
       " 754,\n",
       " 755,\n",
       " 756,\n",
       " 757,\n",
       " 758,\n",
       " 759,\n",
       " 760,\n",
       " 761,\n",
       " 762,\n",
       " 763,\n",
       " 764,\n",
       " 765,\n",
       " 766,\n",
       " 767,\n",
       " 768,\n",
       " 769,\n",
       " 770,\n",
       " 771,\n",
       " 772,\n",
       " 773,\n",
       " 774,\n",
       " 775,\n",
       " 776,\n",
       " 777,\n",
       " 778,\n",
       " 779,\n",
       " 780,\n",
       " 781,\n",
       " 782,\n",
       " 783,\n",
       " 784,\n",
       " 785,\n",
       " 786,\n",
       " 787,\n",
       " 788,\n",
       " 789,\n",
       " 790,\n",
       " 791,\n",
       " 792,\n",
       " 793,\n",
       " 794,\n",
       " 795,\n",
       " 796,\n",
       " 797,\n",
       " 798,\n",
       " 799,\n",
       " 800,\n",
       " 801,\n",
       " 802,\n",
       " 803,\n",
       " 804,\n",
       " 805,\n",
       " 806,\n",
       " 807,\n",
       " 808,\n",
       " 809,\n",
       " 810,\n",
       " 811,\n",
       " 812,\n",
       " 813,\n",
       " 814,\n",
       " 815,\n",
       " 816,\n",
       " 817,\n",
       " 818,\n",
       " 819,\n",
       " 820,\n",
       " 821,\n",
       " 822,\n",
       " 823,\n",
       " 824,\n",
       " 825,\n",
       " 826,\n",
       " 827,\n",
       " 828,\n",
       " 829,\n",
       " 830,\n",
       " 831,\n",
       " 832,\n",
       " 833,\n",
       " 834,\n",
       " 835,\n",
       " 836,\n",
       " 837,\n",
       " 838,\n",
       " 839,\n",
       " 840,\n",
       " 841,\n",
       " 842,\n",
       " 843,\n",
       " 844,\n",
       " 845,\n",
       " 846,\n",
       " 847,\n",
       " 848,\n",
       " 849,\n",
       " 850,\n",
       " 851,\n",
       " 852,\n",
       " 853,\n",
       " 854,\n",
       " 855,\n",
       " 856,\n",
       " 857,\n",
       " 858,\n",
       " 859,\n",
       " 860,\n",
       " 861,\n",
       " 862,\n",
       " 863,\n",
       " 864,\n",
       " 865,\n",
       " 866,\n",
       " 867,\n",
       " 868,\n",
       " 869,\n",
       " 870,\n",
       " 871,\n",
       " 872,\n",
       " 873,\n",
       " 874,\n",
       " 875,\n",
       " 876,\n",
       " 877,\n",
       " 878,\n",
       " 879,\n",
       " 880,\n",
       " 881,\n",
       " 882,\n",
       " 883,\n",
       " 884,\n",
       " 885,\n",
       " 886,\n",
       " 887,\n",
       " 888,\n",
       " 889,\n",
       " 890,\n",
       " 891,\n",
       " 892,\n",
       " 893,\n",
       " 894,\n",
       " 895,\n",
       " 896,\n",
       " 897,\n",
       " 898,\n",
       " 899,\n",
       " 900,\n",
       " 901,\n",
       " 902,\n",
       " 903,\n",
       " 904,\n",
       " 905,\n",
       " 906,\n",
       " 907,\n",
       " 908,\n",
       " 909,\n",
       " 910,\n",
       " 911,\n",
       " 912,\n",
       " 913,\n",
       " 914,\n",
       " 915,\n",
       " 916,\n",
       " 917,\n",
       " 918,\n",
       " 919,\n",
       " 920,\n",
       " 921,\n",
       " 922,\n",
       " 923,\n",
       " 924,\n",
       " 925,\n",
       " 926,\n",
       " 927,\n",
       " 928,\n",
       " 929,\n",
       " 930,\n",
       " 931,\n",
       " 932,\n",
       " 933,\n",
       " 934,\n",
       " 935,\n",
       " 936,\n",
       " 937,\n",
       " 938,\n",
       " 939,\n",
       " 940,\n",
       " 941,\n",
       " 942,\n",
       " 943,\n",
       " 944,\n",
       " 945,\n",
       " 946,\n",
       " 947,\n",
       " 948,\n",
       " 949,\n",
       " 950,\n",
       " 951,\n",
       " 952,\n",
       " 953,\n",
       " 954,\n",
       " 955,\n",
       " 956,\n",
       " 957,\n",
       " 958,\n",
       " 959,\n",
       " 960,\n",
       " 961,\n",
       " 962,\n",
       " 963,\n",
       " 964,\n",
       " 965,\n",
       " 966,\n",
       " 967,\n",
       " 968,\n",
       " 969,\n",
       " 970,\n",
       " 971,\n",
       " 972,\n",
       " 973,\n",
       " 974,\n",
       " 975,\n",
       " 976,\n",
       " 977,\n",
       " 978,\n",
       " 979,\n",
       " 980,\n",
       " 981,\n",
       " 982,\n",
       " 983,\n",
       " 984,\n",
       " 985,\n",
       " 986,\n",
       " 987,\n",
       " 988,\n",
       " 989,\n",
       " 990,\n",
       " 991,\n",
       " 992,\n",
       " 993,\n",
       " 994,\n",
       " 995,\n",
       " 996,\n",
       " 997,\n",
       " 998,\n",
       " 999,\n",
       " ...]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.index_id_to_row_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 141.99it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(['2023年4月1日15时40分许，甲乙双方在济南市天桥区生产路南头附近发生交通事故，造成乙方车辆受损、乙方人员受伤。',\n",
       "   '2023年3月6日12时40分许，房成纪驾驶三轮摩托车行驶至事故地点时，与张元洪驾驶的鲁HZ58C6号轿车相撞，造成房成纪受伤，两车受损。',\n",
       "   '2023年10月21日23时14分许，亓建平驾驶鲁AC32W8号小型客车沿平安路由南向北行驶至力源大街路口时，与由东向西行驶的马传花驾驶的无牌二轮摩托车相撞，造成马传花受伤，两车受伤。',\n",
       "   '2023年4月8日，刘强与冯鑫在聚鑫大酒店门前，因交通事故发生争执，双方产生纠纷。',\n",
       "   '2023年3月13冯强与王传续因车辆碰撞产生纠纷',\n",
       "   '2023年9月29日13时30分许，张震驾驶鲁AD60523号轿车沿雪野通天河大桥由西向东行驶至通天河大桥路口西10米处时，与由南向西左转弯行驶的李瑞驾驶鲁PFD122号二轮摩托车相撞，造成李瑞、乘客谢良进受伤，两车受损。',\n",
       "   '2023年2月26日23时许，王金伟驾驶的鲁ADJ3768号轿车沿文化北路由北向南行驶至颐高附近时，追尾前方行驶的魏述军驾驶的鲁ST0095号出租车，造成两车受损。',\n",
       "   '2023年3月31日17时40分许，潘玉斌驾驶的二轮摩托车沿341国道由东向西行驶至嘶马河桥时，与顺行的时向阳驾驶的鲁S67651号轿车相撞。造成潘玉斌受伤，两车受损。',\n",
       "   '张先生来电反映：9月20日沿北园高架自西往东行驶，前方高架发生事故，两辆事故车发生事故后未及时移动到应急车道，影响后方车辆行驶，造成拥堵，建议今后对于恶意停靠在高架路上的事故车辆进行金额30000元以上罚款，希望相关单位落实处理，请处理。',\n",
       "   '肿瘤医院门前两侧违停问题严重，致使堵车厉害。',\n",
       "   '2023年4月17日8时10分许，彭涛驾驶的鲁A7AG53号轿车在凤凰小区由南向东左转弯行驶至8号楼时，撞至段海霞停放的鲁A02F3G号轿车，造成两车受损。',\n",
       "   '2023年12月1日14时30分许，石茂祯驾驶四轮电动车沿鲁中西大街由西向东行驶至方下镇土楼村路口时，与由南向东右转弯行驶的王霞驾驶的鲁A3T65L号轿车相撞，造成石茂祯受伤，两车受损。',\n",
       "   '2023年6月1日16时50分许，黄斐驾驶鲁A00P9P号面包车沿小罗庄社区门口由东向西行驶至社区门口50米处时，与由西向东行驶的朱玫青驾驶的二轮电动车相撞，造成朱玫青受伤，车辆受损。',\n",
       "   '2023年4月23日17时许，陈正驾驶的鲁SF3308号轿车沿吐丝口路由东向西行驶至口镇卫生院西边十字路口时，与由南向北行驶的魏佑成驾驶的二轮电动车相撞，造成魏佑成受伤。',\n",
       "   '2023年1月10日12时34分许，高伟国驾驶的鲁A35F9V号轿车沿鹿鸣北路由南向北行驶至鲁矿一小东路口时，与由北向南左转弯的刘纯秋驾驶的三轮摩托车相撞，造成刘纯秋受伤，两车受损。',\n",
       "   '2023年3月19日19时25分许，潘震驾驶鲁CL6168（鲁CK865挂）号重型半挂货车沿S234由北向南行驶，行驶至S234济南市莱芜区雪野街道办西站里公交车站路段向西侧变道时，与右前侧同向行驶孙丽驾驶的鲁A13H5P号小型轿车刮撞，轿车失控碰撞路边景观树之后坠入边沟，造成孙丽受伤，两车受损。',\n",
       "   '张女士来电反映：11月5日18时许在天桥区西泺河路与刘家桥街交叉口处，一辆大巴车鲁AH0283在路口未礼让行人，强行通过路口，差点撞到路人，存在安全隐患，要求按照交通法对大巴车进行相应处罚。希望相关单位落实处理，请处理并回复来电人。',\n",
       "   '2023年4月5日17时许，亓瑞国驾驶的鲁SGS982号轿车沿花园路由北向南右转弯行驶至花园学校路口时，与由北向南行驶的雷骐行驾驶的的二轮电动车相撞，造成雷骐行受伤，车辆受损。',\n",
       "   '基本情况：信访人反映6月21日下午桑梓店街道办事处公务人员边某驾驶车辆，在梓东大道鹊山苑路口撞死其家人、撞伤其三岁半孩子，孩子奶奶当场死亡三岁半孩子颅脑严重脑损伤至今无意识。\\n主要诉求：要求解决。',\n",
       "   '温先生来电反映：9月2日在南部山区九顶塔路口发生交通事故，交警进行简易程序，判定为自己全责，导致自己无法走工伤保险，但自己当时为了躲避车辆发生的交通事故，要求重新判定责任，（鲁AE2320）（通话中来电人表示马上到申诉时限，要求尽快处理）希望相关单位落实处理，请处理并回复来电人。',\n",
       "   '2023年1月13日，冯崇治驾驶鲁A32D35小型轿车在济南市槐荫区清源路与腊山河西路左转时与王德将驾驶的山东于蓝电动车有限公司所有的电动车发生交通事故，事故造成两车受损，无人员伤亡',\n",
       "   '2023年12月22日15时40分许，王本康驾驶鲁A31K6T号轿车沿金桥花园路段由西向东行驶至金桥花园北门时，刮至杨锦绣停放的鲁SAF088号轿车，造成车辆受损。',\n",
       "   '来电人反映：4月22日6:46在莱芜高新区和园小区红绿灯路口，一辆公交，车牌：鲁S52211，在自己车辆正常拐弯时公交快速拐弯，差点发生碰撞，来电投诉。希望相关单位落实处理，请处理。',\n",
       "   '来电人反映：鲁AD50616，2023年5月22日14：00乘车，14：50至目的地长途汽车站，行程中驾驶员车速极快，行驶至北园高架欧亚家居附近时驾驶员超车、别拐、连续变三车道、开斗气车险些与其他社会车辆发生交通事故，置乘客安全于不顾，现对此不满来电投诉，希望相关单位落实处理，请处理。',\n",
       "   '小清河北岸渣土车导致道路破损影响通行',\n",
       "   '我是赵国启，我反映的问题是国道309修路造成扬尘和堵车，给老百姓出行造成不便',\n",
       "   '社区外道路两侧小商小贩占道经营，存在交通方面的安全隐患',\n",
       "   '来电人反映：8月15日11:45，在槐荫区经十路与腊山北路交叉路口（世购路口林肯汽车服务中心）经过信号灯，有一辆银色面包车没有礼让行人由东往北右转速度很快差点撞到行人，不记得车牌号，要求举报该车辆并对该车辆做出处罚。希望相关单位落实处理，请处理。',\n",
       "   '路先生来电反映：2023年5月29日11:20左右自己驾驶车辆鲁A8C21S由东向西在经十一路上直行，571路公交车（AH2379）从迎宾路往经十一路左转，由南向西，没有及时观察后方车辆，导致差点发生交通事故，如果不是自己刹车及时，会发生碰撞，并且司机速度非常快、超速、此处经过路口也没有礼让行人，要求对于违法行为进行处理、对司机进行处理。（要求公交公司答复和处理)。希望相关单位落实处理，请处理并回复来电人。',\n",
       "   '2023年12月21日11时许，李冰驾驶鲁S21201号轿车沿大崮山路由南向北行驶至聚丰园饭店门口时，刮至路边张建伟停放的鲁A58T6Z号轿车，造成车辆受损。',\n",
       "   '2023年1月20日23时30分许，杨帅驾驶的鲁A62WY2号双排货车沿口镇街道办东街小区门口停车场由北向南倒车时，撞至景茂全停放的鲁SM3033号轿车，造成两车受损。',\n",
       "   '有车辆乱停堵塞消防通道影响交通安全问题，要求办事处尽快解决',\n",
       "   '损害赔偿纠纷',\n",
       "   '梁府小区门口和西边新修的路上全是不自觉乱停乱放的汽车，上下班点电动车都堵得蹬蹬的，严重影响交通',\n",
       "   '张先生来电反映：来电人要求原话记录：5月6日16:03左右来电人骑行自行车在平阴县国道341线368公里350米附近发生交通事故，平阴县公安局交通警察大队第370124120230000070号案件，交警判定双方同等责任，第一实际肇事司机在经过路口时存在明显的超速行为，车速达到70千米每小时，第二大货车行驶在超车道，第三，大货车刹车失灵，存在安全隐患，通过这三点原因认为对方应该负全部或主要责任，来电人实际是直行并且观察充分，并非判决书上写的左转方向，责任书上写着来电人没有仔细观察，来电人认为这一点是错误的，当时有一辆渣土车经过，来电人无法观察到对面车道，来电人已经避让渣土车，说明来电人有安全意识，对于交警处理结果不服，交警明显存在偏袒，偏袒对方，来电投诉，来电人没有钱请律师。要求相关部门帮助主持公道，要求有关负责人尽快落实，（对方车牌：鲁PR6317）希望相关单位落实处理，请处理并回复来电人。',\n",
       "   '艾先生来电反映：是槐荫区玉清湖街道田家庄村村民，2021年7月31日在槐荫区烟台路西口与一个大车发生交通事故，槐荫交警大队受理，交警告知没有监控让去诉讼，没有划分责任，并且此处有限速牌子，对方车辆超速（67公里）逆行，交警也没有监管，认为不合理，要求交警重新核实处理。8月1日11:00接到交警工作人员用（66666122）给回复，告知没有交通信号灯无法判定谁责任，实际自己左转进入车道走七八米之后，对方车辆逆行自己行驶车道里面，发生交通事故，造成自己十根肋骨断裂，来电求助 ，要求交警重新核实，重新定责，希望相关单位落实处理，请处理并回复来电人。',\n",
       "   '荷花路街道孟家村西高速入口堵车问题严重了，每天都堵车，严重影响出行',\n",
       "   '郑允娟女士来电反映：12月11日15:00多在奥体西路与华龙路交叉口红绿灯旁发生交通事故，自己骑电动车，对方是机动车（不清楚对方车牌号）12月12日报警到高新区交警，交警工作人员（18615316691）告知已经查询到对方车牌号，但是联系不上对方，目前还在住院，希望交警尽快处理这个事故，希望相关单位落实处理，请处理并回复来电人。',\n",
       "   '井先生来电反映：自己母亲发生交通事故被肇事者的三轮车压倒，但天桥区泺口交警到达现场后并未帮助母亲录口供，并且自己母亲出院后非让自己带母亲去到泺口交警大队事故处理科录口供，录口供时陈姓男性交警态度不好，并且处理结果不合理，鉴定结果还未出来，就将肇事者放行，认为不合理，来电投诉此位交警。希望相关单位落实处理，请处理并回复来电人。',\n",
       "   '孙女士来电反映：7月21日16:40左右驾驶机动车从高新区机场西路路口左拐，已经确定当时没有直行车辆，马上就要拐过弯去，因为自己是一名孕妇，所以已经观察好路况并缓慢行驶拐弯，但是有一辆出租车突然加速从车辆右侧直行通过，且该司机通过后刹车急停对自己进行辱骂，险些撞到出租车上，认为不合理来电投诉，要求出租车公司对该司机进行处理。（出租车比亚迪鲁AD65539）希望相关单位落实处理，请处理并回复来电人。',\n",
       "   '史先生来电反映：济阳区仁风镇邢家村村口的位置经常发生交通事故，希望在此处设置信号灯。希望相关单位落实处理，请处理并回复来电人。',\n",
       "   '2021年4月9日7时30分许，被告驾驶鲁ST0328号小型客车行至事故地点时，与原告钱丽驾驶的电动自行车发生事故，造成原告受伤。交通警察支队对该次交通事故作出责任认定，认定被告孔祥军在此次交通事故中全部责任，原告不承担责任。另查明被告所驾驶的车辆在被告泰山财产保险股份有限公司莱芜中心支公司投保，为维护原告的合法权益，依法诉至贵院，请求法院依法判决，以实现原告的诉讼请求。',\n",
       "   '刘先生来电反映：2023年9月29日10:08在长清区孝里北上高速口后向泰安、菏泽方向行驶混入主道后，有交警在之前，自己前面的大货车别自己的车辆，自己停车向交警反映，交警称两车没有发生交通事故不予处理，认为不合理，来电投诉交警的处理方式。希望相关单位落实处理，请处理并回复来电人。',\n",
       "   '张先生来电反映：钢城区双泉路桥洞下,道路破损，路面上有坑，开车在此处经过，导致自己车辆受损，要求给自己赔偿。希望相关单位落实处理，请处理并回复来电人。',\n",
       "   '刘先生来电反映：交通警察支队历下区大队给自己开具道路交通事故认定书3701021202300003951，有歧义，自己是行人，对方是非机动车，给判定是同等责任，认为不合理，要求交通警察支队给自己重新判定，对方全责。希望相关单位落实处理，请处理并回复来电人。',\n",
       "   '西门禁止陌生车辆进入，导致生活不便',\n",
       "   '吴女士来电反映：4月2日在天桥区八里桥堤口路由西向东方向在幸福时光KTV门口处发生交通事故（两个电瓶车），自己正常行驶，因对方超速没有留安全距离导致自己韧带及半月板损伤两周无法下床在医院花费大约2000多，对方没有受伤，自己报警交警协商对方给与自己300元赔偿自己不认可，之后对方全权交给天桥区交警大队处理，结果交警偏袒肇事者，将全责判给自己肇事者不出面解决问题，希望肇事者给与赔偿承担相应的责任，并希望天桥区交警大队给与公平公正处理。（肇事者电话15069080590）希望相关单位落实处理，请处理并回复来电人。',\n",
       "   '牛女士来电反映：7月8日17:50分左右在试验机厂至济南大学路段，来电人在最右侧直行车道行驶，但K301路公交车变道时不关注后方行驶车辆，存在恶意别车现象，将来电人挤出车道，造成来电人差点摔倒，来电人表示K301车队驾驶员普遍车速快，变道时不关注后方行驶车辆，来电投诉，要求对此进行查处整改，希望相关单位落实处理，请处理并回复来电人。',\n",
       "   '接孩子放学的路口，晾晒玉米影响通行，而且他还摆放了很多砖头和木棍等，严重影响车辆安全，还容易引起车辆事故，希望领导能妥善处理。',\n",
       "   '网友微信反映： 投诉郭店街道交通相关问题：10月27号到11月2号机场路稼轩路虞山路发生三起交通事死亡三人，要求虞山路，稼轩路，禁行大车，希望相关单位落实处理，请处理。',\n",
       "   '刘先生来电反映：2023年11月3日驾驶电动车在天桥区世贸天城和名泉春晓十字路口由西往东方向行驶，当时指挥交通人员未指挥交通，导致自己头，眉角，腿胳膊摔倒，流很多血，要求相关部门给予赔偿，（来电人表示不需要联系120）希望相关单位落实处理，请处理并回复来电人。',\n",
       "   '网友“李”微信反映：2023年12月26日下午六点左右，在二环东路与华龙路山大南路交叉的大路口，本人行驶至此，绿灯行驶出路口当天值班交警手势叫停，让对向左转车行驶，待行驶完后前方已无车辆且仍绿灯交警也已不在此处，本人正常行驶至马路对面（由华龙路至山大南路）且本人旁边及后续车辆也有跟随，至路口处因有右转车量及行人在路口处停车等待，该交警走至本人车旁，态度恶劣，质问本人知道为什么叫你停车吗，指责因本人影响交通堵塞。首先本人行驶时前方已无任何车辆（前期左转车也已行驶完毕）且该交警也未做停止手势，本人行驶时为绿灯通行。第二，停至路口因右转车辆等待行人通过，与本人有何关系，且同时行驶有其他同行车辆，为何不找？投诉该交警态度差，要求赔礼道歉。希望相关单位落实处理，请处理并回复诉求人。',\n",
       "   '曹先生来电反映：鲁AD73328，3月5日22:00在环联夜市门口该车急刹车，变道不打转向灯，不文明行驶，对此表示不满，要求进行处理，希望相关单位落实处理，请处理。',\n",
       "   '彭先生来电反映：8月22日19:11驾驶车辆（车牌号：鲁A01636D）由南往北经过历城区工业南路殷陈铁路桥下时，一个细长的铁质条状物被别的车压到后崩起砸到自己车辆，导致车辆前挡风玻璃左前角损坏，要求给予赔偿。希望相关单位落实处理，请处理并回复来电人。',\n",
       "   '来电人反映：鲁ADM5395，8月23日12：27在济南西站到齐鲁软件园，该车严重超速，不打转向灯，音响声音开的特别大，存在很大的安全隐患，来电投诉司机，希望相关单位落实处理，请处理。',\n",
       "   '张先生来电反映：12月1日19点多驾驶鲁A2W6B9在天桥区北园高架行驶时，车灯被前方拖车掉落的物品砸坏，警方表示监控太模糊，看不清车牌号，希望警方进一步调查。希望相关单位落实处理，请处理并回复来电人。',\n",
       "   '张先生来电反映：高新区大正路向右转至世纪大道中大药业附近，2023年3月7日07:12左右因道路有一处大约半平方米左右的大坑（不清楚多深），车辆行驶到此处压到坑后出现轮胎爆胎，轮毂受损的情况，现在车辆不能行驶。现在大约有十几辆车无法通过，要求落实车辆受损由谁承担责任并要求赔偿。希望相关单位落实处理，请处理并回复来电人。',\n",
       "   '张先生来电反映：1.天桥区北园大街与济洛路由西向东直行有两条车道左右拐车道各两条，导致直行车道堵车严重，建议将右转车道更改为一条车道。\\n2.天桥区无影山东路与北园大街交叉口由西向东只有一条直行车道，左转有两条车道，导致督促严重，建议将左转车道更改为一条车道或者更改为可变车道。希望相关单位落实处理，请处理并回复来电人。',\n",
       "   '钟女士来电反映：8月10日13:55分在长清区平安南路与海棠路交叉口位置由北向南通行，自己准备在最左侧车道调头，自己准备变道时后面车辆未减速差点与自己发生剐蹭，对方追上自己后将自己车辆别停，来电投诉，要求相关单位尽快调查进行处罚（鲁A96JE2）。希望相关单位落实处理，请处理并回复来电人。',\n",
       "   '匿名先生来电反映：天桥区新黄路和舜鑫中路交叉口，自己车辆鲁A266LM被剐蹭，联系交警调取监控，交警告知监控故障，无法取证，非常不合理，来电投诉，要求交警帮助落实调取监控处理问题。希望相关单位落实处理，请处理并回复来电人。',\n",
       "   '王先生来电反映：3月25日8:27自己母亲在章丘区潘王路与郭家村路口交叉处发生交通事故，车牌号鲁A27UH9，事故责任交警是章丘交警大队埠村中队赵灿辉，只听信违法人员一面之词，耽误对于违法人员的处理进度，徇私舞弊，来电投诉该交警，要求埠村中队对于违法人员进行处理。（应来电人要求提供电话）希望相关单位落实处理，请处理并回复来电人。',\n",
       "   '董先生来电反映：4月28日在章丘区经十东路与章丘石河街路口发生摩托车（鲁ADS685 李洁璐）和自己对象电动车剐蹭一案，章丘交警大队告知让5月6日去交警大队查监控，查到监控后，交警大队找不到肇事车主，给该车主打电话询问该车已售出，无法联系肇事车主，认为不合理，要求相关部门找出肇事车主并要求该肇事车主提供车辆的全部车架号和发动机号。希望相关单位落实处理，请处理并回复来电人。',\n",
       "   '陈女士来电反映：2023年2月17日18:00左右在二环西路和北外环交叉口发生剐蹭事故，与对方车辆一起到天桥区泺口交警队处理，肇事司机在交警队逃跑了，对此不满，来电投诉交警大队，要求给予合理答复。希望相关单位落实处理，请处理并回复来电人。',\n",
       "   '济南市历城区郭店街到办事处李西小区消防通道被机动车占用，影响出行，具有严重安全隐患，要求有关部门清理。',\n",
       "   '刘先生来电反映：章丘区龙泉大道与绣江路交叉口红绿灯，夜间有大货车闯红灯，要求交警部门进行查处治理。希望相关单位落实处理，请处理并回复来电人。',\n",
       "   '罗先生来电反映：10月5日在章丘区相公庄发生交通事故，章丘区交警知识拍了几张照片，什么都没做，就将车拖走，并收取200元道路救援费，认为不合理，要求将费用退还。（鲁AFH883）希望相关单位落实处理，请处理并回复来电人。',\n",
       "   '刘先生来电反映：商河县徐四村路口，2月1日发生的车辆刮蹭事故，对方车辆没有牌照且对方无证驾驶，交警出警态度不好，未对对方作出处罚，来电投诉，希望相关单位落实给予合理解释（来电人车牌鲁A8y958），希望相关单位落实处理，请处理并回复来电人。',\n",
       "   '来电人反映：起步区大桥镇邢家村有大车从村内经过将道路压坏，并且行驶噪音扰民严重，希望大车不要在村里经过。（现在正在干活）希望相关单位落实处理，请处理。',\n",
       "   '2023年5月2日13时许，唐崇义与贡小平因停车问题发生争执，产生纠纷。',\n",
       "   '朱先生来电反映：4月份在工业南路自己驾驶小型客车跟一辆摩托车撞了，交警一直没有划分责任，证据不足，表示鉴定不出来车速，第二次也没有测出车速，说对方没有改装车辆，但之后又说车辆与行驶证不相符，第一次交警没有说对方改装尾箱，交警也没有解释自己提出的问题，维护对方，交警部门还告知就算对方即时没有证有技术就行，对此不满，要求交警部门告知发生交通事故地点最高限速是多少以及合理解释。（车牌号：鲁KN036K）希望相关单位落实处理，请处理并回复来电人。',\n",
       "   '临近过秋，有些村民不顾危险，将刚刚收割的庄家铺在顺政路道路上，影响车辆正常通行，要求及时快速恢复道路的正常通行功能。我是郑家村 苏明孟 ，要求尽快恢复道路正常通行。',\n",
       "   '何某其在自己承包地打农药，喷到李某某地中，导致李某某农作物受损，双方发生争议。',\n",
       "   '宋先生来电反映：11月初左右济阳区中国人民银行路口，自己骑行电动车直行，对方骑行有牌的四轮电动车转弯跟自己出现交通事故，自己受伤电动车受损，报警后济阳区交警事故科出警，车辆都被扣押到了停车场，现在交警没有出具任何责任认定书，对方没有给自己赔偿医疗费、没有跟自己协商的情况下，交警就让对方把车辆提走，对此不满，来电投诉，要求对车辆进行扣押，出具责任认定书。希望相关单位落实处理，请处理并回复来电人。',\n",
       "   '来电人反映：2023年8月2日19:38:25左右北向南自驾在第一车道通行至舜耕路时，自编号2-0828的B76路驾驶员不安全驾驶，在第二车道恶意别车，对此不满来电投诉，希望相关单位落实处理，请处理。',\n",
       "   '网友“王先生”微信反映：章丘明水汇泉路东段秀水安置房以南路段，占用非机动车道，人行道随意摆摊致使接孩子上学放学走机动车道，万一出现交通事故怎么办？非机动车道摆东西满满的根本没法正常行驶。希望领导们管一管！希望相关单位落实处理，请处理。',\n",
       "   '来电人反映：匿名投诉济南市商河县商西路与辛庄街交叉口以南东侧，到处都是乱停放的车辆，严重影响过往车辆通行，容易引发交通事故，望有关部门严肃处理！严惩不贷！ 希望相关单位落实处理，请处理。',\n",
       "   '因邻里矛盾产生的纠纷',\n",
       "   '董先生来电反映：鲁A03632D，9月26日15:23在经一纬二东侧经一路上，由西向东行驶，来电人变道，司机未让，将自己的反光镜剐蹭，来电人按喇叭，司机仍继续行驶，有视频，要求投诉司机并电话道歉。希望相关单位落实处理，请处理并回复来电人。',\n",
       "   '因浇地出现的矛盾引发的纠纷',\n",
       "   '我是天桥区的高凡茹，院领导重视此事并解决此事',\n",
       "   '李某与孔某因误会发生争吵',\n",
       "   '李先生来电反映：南部山区西营街道大南营村通往跑马岭的路口位置经常发生车祸，建议在路口前方的位置安装减速带。希望相关单位落实处理，请处理并回复来电人。',\n",
       "   '网友“陈老师”微信反映：投诉济南交警市级支队对槐荫区交警大队没有起到监督督查的作用，造成槐荫区交警大队存在交通标志设置混乱，侵犯了人民群众（本人及其他受害人）财产及其合法权益，累计罚款较大（本人及其他受害人合计），违反了《中华人民共和国民法典》、《中华人民共和国道路交通安全法》。\\n1、根据《中华人民共和国道路交通安全法》第二十五条 全国实行统一的道路交通信号。交通信号包括交通信号灯、交通标志、交通标线和交通警察的指挥。交通信号灯、交通标志、交通标线的设置应当符合道路交通安全、畅通的要求和国家标准，并保持清晰、醒目、准确、完好。根据通行需要，应当及时增设、调换、更新道路交通信号。增设、调换、更新限制性的道路交通信号，应当提前向社会公告，广泛进行宣传。本次处罚是在“交通标志不清晰、不醒目、不完好，未做到提前向社会公告，广泛进行宣传”的前提下做出的处罚，故本处罚违违反了本条法则。\\n2、根据《中华人民共和国道路交通安全法》第三十六条  根据道路条件和通行需要，道路划分为机动车道、非机动车道和人行道的，机动车、非机动车、行人实行分道通行。没有划分机动车道、非机动车道和人行道的，机动车在道路中间通行，非机动车和行人在道路两侧通行。该处罚点根据本条“没有划分机动车道、非机动车道和人行道的，非机动车和行人在道路两侧通行”中提到的“非机动车和行人在道路两侧通行”，两侧是指辅道还是辅道右侧的人行道，无法准确判定，故在没有明确提示的前提下，无法判定辅道是机动车道还是非机动车道，故本处罚违反了本条法则。\\n3、本次处罚是车停在辅道收到短信提醒即将违法请驶离，然后开车驶离被路口抓拍，存在重大故意引导群众违法嫌疑，违反了《中华人民共和国民法典》第八条 民事主体从事民事活动，不得违反法律，不得违背公序良俗。故槐荫交警大队存在重大“违背公序良俗”嫌疑。\\n4、在不满足以上条的前提下，对群众做出处罚，违反了《中华人民共和国民法典》第三条 民事主体的人身权利、财产权利以及其他合法权益受法律保护，任何组织或者个人不得侵犯。故槐荫交警大队侵犯了人民群众（本人及其他受害人）财产及其合法权益。希望相关单位落实处理，请处理并回复诉求人。',\n",
       "   '邢先生来电反映：建议公开济南市各区交警大队电话。希望相关单位落实处理，请处理。',\n",
       "   '刘先生来电反映：2023年8月13日下午17：00多骑乘电动车2人在莱芜区龙源宾馆附近十字路口，（附近有一个加汽站）被两辆车撞，被鲁A0R08W 和鲁B1441S撞， 骑车人已经死亡 ，另外一个受重伤丈母娘（在重症监护室），一天医药费好几万元，（老丈脑血栓，一家人生活全靠丈母娘维持，生活非常困难），现已经无法支付医药付，当时已经报案（只知道交警手机19863488060无法提供报案号码），问题一直有处理结果，要求肇事方给支付医药费和赔偿问题，希望相关单位落实处理， 请处理并回复来电人。',\n",
       "   '王建浩省级网站反映：我是济南大学的学生，我要举报这辆车在人行道路上超速行驶，并且不礼让行人，不鸣笛警示行人不减速，差点撞到人，我在口头提示司机慢一点后居然停下车威胁我且辱骂我，对我和我舍友造成严重心理阴影。地点在后龙庄小区小吃街入口处，时间为晚上八点28分前后。车牌为鲁Av27L3。希望相关单位落实处理，请处理并回复诉求人。',\n",
       "   '来电人反映：高新区浪潮集团工地的建筑车辆已经堵满辅道，而且自己通行机动车道上有一辆车（鲁AF96NR）一直在按喇叭，对此不满，来电投诉，要求交警部门及时查处。希望相关单位落实处理，请处理。',\n",
       "   '边先生来电反映：自己是客车司机，从旅游路上兴隆高速时被抓拍，认为不允许大型客车通行旅游路不合理，希望可以正常通行。希望相关单位落实处理，请处理并回复来电人。',\n",
       "   '来电人反映：在3月31日23:00张庄路前屯小区东侧，交警查到槐荫区孔家村村书记醉驾，并没有拘留村书记，交警部门到底是如何处理这起醉驾的，来电咨询为何没有处理村书记醉驾行为。（黑色车辆，具体不清楚车牌号）\\n市交警支队回复：根据法律规定，案件是不对非当事人或委托人告知的，如来电人对事故处理有疑问，可以随时到槐荫区交警大队肇事处理中队咨询，也非常感谢其对公安交通管理工作的支持、理解，希望平安出行。\\n来电人再次来电反映：投诉槐荫区肇事科，交警部门徇私舞弊，要求告知此人按规定是否已处理。（表示再得不到满意的答复就到政法委投诉肇事科）希望相关单位落实处理，请处理。',\n",
       "   '宗先生来电反映：槐荫区山东省耳鼻喉医院西门段北东路，道路上经常违停严重，造成交通堵塞，希望交警部门加大查处力度。希望相关单位落实处理，请处理并回复来电人。',\n",
       "   '马先生来电反映：在经十西路槐荫区政府门口有一个大坑，自己开车行驶时路面有一个大坑把自己的车轮圈掂坏了，要求尽快把坑恢复。希望相关单位落实处理，请处理并回复来电人。',\n",
       "   '来电人反映：章丘区龙山街道宋家埠村村民，在宋家埠村与大桑树村交接处，有大货车（没有车牌）横在马路上，导致无法正常通行，要求相关部门挪移车辆，希望相关单位落实处理，请处理。',\n",
       "   '历城区东兴花园小区内很多机动车长期乱停放，导致其他非机动车通行困难，要求管理机动车乱停放，希望相关单位落实处理的问题。',\n",
       "   '张先生来电反映：天桥区生产路延长线存在很多车停放现象，严重影响出行，来电投诉天桥交警不作为。希望相关单位落实处理，请处理并回复来电人。',\n",
       "   '网友微信反映：建议有关部门把济泺路与泺安路交叉口，南向北方向再加一个直行车道进黄河隧道，左转有两个车道有点浪费，左转车很少.希望相关单位落实处理，请处理。',\n",
       "   '匿名女士来电反映：市中区经十路与青年东路交叉口，由西向东方向，车牌为鲁JP056挂的车辆行驶速度过快，占用两个车道，来电投诉，要求交警部门查处整治。希望相关单位落实处理，请处理。',\n",
       "   '我是制锦市街道居民，制锦市街道锦屏街存在大量的僵尸车占用公共车位，幼儿园接送孩子无停车位停车造成交通堵塞存在安全隐患。要求制锦市街道整治僵尸车辆占用公共车位问题，排除交通隐患。',\n",
       "   '郭先生来电反映：莱芜区高庄交警中队向东50米的交叉口，此处没有信号灯，是事故频发路段，建议在此处安装信号灯。希望相关单位落实处理，请处理并回复来电人。',\n",
       "   '王先生来电反映：7月8日23:35左右在花园东路至二环东高架位置，自己车辆正常行驶，因为信号灯前方车辆停车自己也停车，遭到出租鲁AD78736辱骂，且绿灯之后出租司机不顾交通追着自己进行二次辱骂，给济南抹黑，来电投诉。希望相关单位落实处理，请处理并回复来电人。',\n",
       "   '杨女士来电反映：济阳区银河路希尔顿大酒店西侧红绿灯向南路面损坏严重，影响车辆通行，要求尽快修复路面。希望相关单位落实处理，请处理并回复来电人。'],\n",
       "  array([197.70512, 196.87627, 193.21593, 192.94562, 192.52884, 192.24054,\n",
       "         191.62143, 191.07886, 188.33331, 187.70297, 187.66278, 187.53961,\n",
       "         187.45253, 186.80829, 186.78549, 185.89107, 185.70209, 185.31679,\n",
       "         184.6548 , 182.94446, 181.99629, 181.80452, 181.01724, 181.00993,\n",
       "         180.58804, 180.313  , 179.87471, 179.6652 , 179.4712 , 178.45674,\n",
       "         178.42487, 178.41045, 178.16553, 176.04211, 175.96236, 175.90657,\n",
       "         175.73346, 175.5889 , 175.36406, 175.2658 , 175.15167, 174.88019,\n",
       "         174.7083 , 174.29271, 174.15208, 173.82185, 173.64502, 173.30075,\n",
       "         172.42978, 172.13937, 171.8746 , 171.27989, 171.2173 , 171.06615,\n",
       "         170.81587, 170.79234, 170.50616, 170.40308, 170.30501, 170.28256,\n",
       "         170.01543, 169.83014, 169.67332, 169.59137, 169.57245, 169.56409,\n",
       "         169.51949, 169.3951 , 169.31372, 169.24232, 169.11221, 169.0641 ,\n",
       "         168.77051, 168.44214, 168.40479, 168.36258, 168.1449 , 168.02982,\n",
       "         167.97398, 167.85976, 167.74388, 167.61627, 167.57419, 167.54587,\n",
       "         167.51485, 167.39124, 167.35225, 167.24445, 167.1896 , 167.13449,\n",
       "         167.09184, 167.07098, 167.04523, 166.98582, 166.88657, 166.81042,\n",
       "         166.7321 , 166.64554, 166.56548, 166.55394], dtype=float32))]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.search_return_text(\"严重的交通事故\",100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['如何更换花呗绑定银行卡', '花呗更改绑定银行卡', '火影忍者真好看', '火影忍者作者画的挺好的']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame = pd.read_excel('retriever_test.xlsx')\n",
    "data_frame.iloc[0]['描述']\n",
    "ids = data_frame['描述'].tolist()\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized FAISS index of type <class 'faiss.swigfaiss.IndexFlatIP'>\n",
      "Loading Excel file...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 133\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m    132\u001b[0m retriever \u001b[38;5;241m=\u001b[39m ExcelIndexer(vector_sz\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m768\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel, embeddings_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mterm_work/embeddings.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 133\u001b[0m \u001b[43mretriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mterm_work/更新后的网格文件_提取矛盾类型和严重程度_提取区名.xlsx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m描述\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 29\u001b[0m, in \u001b[0;36mExcelIndexer.load_excel\u001b[0;34m(self, excel_file, id_column, batch_size)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# 加载 Excel 文件到 DataFrame\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoading Excel file...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_frame \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexcel_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoaded Excel file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexcel_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, total rows: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_frame)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     31\u001b[0m lenth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_frame)\n",
      "File \u001b[0;32m~/miniconda3/envs/retriever/lib/python3.9/site-packages/pandas/io/excel/_base.py:508\u001b[0m, in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    503\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    504\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    505\u001b[0m     )\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 508\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m        \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m        \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrue_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrue_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfalse_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfalse_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mna_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_default_na\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_default_na\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mna_filter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_filter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdate_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_parser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthousands\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthousands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecimal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecimal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskipfooter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipfooter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    534\u001b[0m     \u001b[38;5;66;03m# make sure to close opened file handles\u001b[39;00m\n\u001b[1;32m    535\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m should_close:\n",
      "File \u001b[0;32m~/miniconda3/envs/retriever/lib/python3.9/site-packages/pandas/io/excel/_base.py:1616\u001b[0m, in \u001b[0;36mExcelFile.parse\u001b[0;34m(self, sheet_name, header, names, index_col, usecols, converters, true_values, false_values, skiprows, nrows, na_values, parse_dates, date_parser, date_format, thousands, comment, skipfooter, dtype_backend, **kwds)\u001b[0m\n\u001b[1;32m   1576\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\n\u001b[1;32m   1577\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1578\u001b[0m     sheet_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1596\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds,\n\u001b[1;32m   1597\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, DataFrame] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mint\u001b[39m, DataFrame]:\n\u001b[1;32m   1598\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1599\u001b[0m \u001b[38;5;124;03m    Parse specified sheet(s) into a DataFrame.\u001b[39;00m\n\u001b[1;32m   1600\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1614\u001b[0m \u001b[38;5;124;03m    >>> file.parse()  # doctest: +SKIP\u001b[39;00m\n\u001b[1;32m   1615\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1617\u001b[0m \u001b[43m        \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1619\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1620\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1621\u001b[0m \u001b[43m        \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1622\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1623\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrue_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrue_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1624\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfalse_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfalse_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mna_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdate_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_parser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthousands\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthousands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskipfooter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipfooter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1635\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1636\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/retriever/lib/python3.9/site-packages/pandas/io/excel/_base.py:778\u001b[0m, in \u001b[0;36mBaseExcelReader.parse\u001b[0;34m(self, sheet_name, header, names, index_col, usecols, dtype, true_values, false_values, skiprows, nrows, na_values, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, dtype_backend, **kwds)\u001b[0m\n\u001b[1;32m    775\u001b[0m     sheet \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_sheet_by_index(asheetname)\n\u001b[1;32m    777\u001b[0m file_rows_needed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calc_rows(header, index_col, skiprows, nrows)\n\u001b[0;32m--> 778\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_sheet_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43msheet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_rows_needed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(sheet, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclose\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;66;03m# pyxlsb opens two TemporaryFiles\u001b[39;00m\n\u001b[1;32m    781\u001b[0m     sheet\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/retriever/lib/python3.9/site-packages/pandas/io/excel/_openpyxl.py:615\u001b[0m, in \u001b[0;36mOpenpyxlReader.get_sheet_data\u001b[0;34m(self, sheet, file_rows_needed)\u001b[0m\n\u001b[1;32m    613\u001b[0m data: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[Scalar]] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    614\u001b[0m last_row_with_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 615\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row_number, row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sheet\u001b[38;5;241m.\u001b[39mrows):\n\u001b[1;32m    616\u001b[0m     converted_row \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_cell(cell) \u001b[38;5;28;01mfor\u001b[39;00m cell \u001b[38;5;129;01min\u001b[39;00m row]\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m converted_row \u001b[38;5;129;01mand\u001b[39;00m converted_row[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    618\u001b[0m         \u001b[38;5;66;03m# trim trailing empty elements\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/retriever/lib/python3.9/site-packages/openpyxl/worksheet/_read_only.py:96\u001b[0m, in \u001b[0;36mReadOnlyWorksheet._cells_by_row\u001b[0;34m(self, min_col, min_row, max_col, max_row, values_only)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# return cells from a row\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m counter \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m idx:\n\u001b[0;32m---> 96\u001b[0m     row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m     counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m row\n",
      "File \u001b[0;32m~/miniconda3/envs/retriever/lib/python3.9/site-packages/openpyxl/worksheet/_read_only.py:125\u001b[0m, in \u001b[0;36mReadOnlyWorksheet._get_row\u001b[0;34m(self, row, min_col, max_col, values_only)\u001b[0m\n\u001b[1;32m    123\u001b[0m         new_row[idx] \u001b[38;5;241m=\u001b[39m cell[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m values_only:\n\u001b[0;32m--> 125\u001b[0m             new_row[idx] \u001b[38;5;241m=\u001b[39m ReadOnlyCell(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcell)\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(new_row)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "class ExcelIndexer(object):\n",
    "    def __init__(self, vector_sz: int, n_subquantizers=0, n_bits=8, model: SentenceModel = None, embeddings_file: str = None):\n",
    "        \"\"\"初始化索引器，选择使用FAISS的类型\"\"\"\n",
    "        if n_subquantizers > 0:\n",
    "            self.index = faiss.IndexPQ(vector_sz, n_subquantizers, n_bits, faiss.METRIC_INNER_PRODUCT)\n",
    "        else:\n",
    "            self.index = faiss.IndexFlatIP(vector_sz)\n",
    "        self.index_id_to_row_id = []  # 用于存储 FAISS 索引 ID 到 Excel 文件中的行号映射\n",
    "\n",
    "        self.data_frame = None  # 存储 Excel 数据\n",
    "        self.model = model\n",
    "        self.embeddings_file = embeddings_file  # 存储嵌入向量文件路径\n",
    "\n",
    "        print(f'Initialized FAISS index of type {type(self.index)}')\n",
    "\n",
    "    def load_excel(self, excel_file: str, id_column: str, batch_size: int = 2048):\n",
    "        \"\"\"\n",
    "        加载 Excel 文件并自动创建 FAISS 索引\n",
    "        :param excel_file: Excel 文件路径\n",
    "        :param vector_column: 存储嵌入向量的列名\n",
    "        :param id_column: 存储唯一标识符的列名\n",
    "        \"\"\"\n",
    "        # 加载 Excel 文件到 DataFrame\n",
    "        print('Loading Excel file...')\n",
    "        self.data_frame = pd.read_excel(excel_file)\n",
    "        print(f'Loaded Excel file: {excel_file}, total rows: {len(self.data_frame)}')\n",
    "        lenth = len(self.data_frame)\n",
    "\n",
    "        # 如果存在保存的嵌入向量文件，直接加载它\n",
    "        if self.embeddings_file and os.path.exists(self.embeddings_file):\n",
    "            print(f'Loading embeddings from {self.embeddings_file}...')\n",
    "            self.embeddings = np.load(self.embeddings_file)\n",
    "            print(f'Loaded embeddings from file, shape: {self.embeddings.shape}')\n",
    "        else:\n",
    "            self.embeddings = np.empty((lenth, self.index.d), dtype=np.float32)  # 初始化一个空的嵌入向量矩阵\n",
    "            # 提取 ID 和向量数据\n",
    "            print('Indexing data...')\n",
    "            for times in range(lenth // batch_size + 1):\n",
    "                print(f'Indexing batch {times + 1}/{lenth // batch_size + 1}, total indexed: {len(self.index_id_to_row_id)}, total data: {lenth}')\n",
    "                start = times * batch_size\n",
    "                end = min((times + 1) * batch_size, lenth)\n",
    "                ids = range(start, end)\n",
    "                embeddings = np.array([self.model.encode(self.data_frame[id_column][i]) for i in range(start, end)]).astype('float32')\n",
    "\n",
    "                # 保存嵌入向量到内存中\n",
    "                self.embeddings[start:end] = embeddings\n",
    "                self.index_data(ids, embeddings)\n",
    "\n",
    "            # 保存嵌入向量到文件\n",
    "            if self.embeddings_file:\n",
    "                print(f'Saving embeddings to {self.embeddings_file}...')\n",
    "                np.save(self.embeddings_file, self.embeddings)\n",
    "\n",
    "        print('Indexing done!')\n",
    "\n",
    "    def index_data(self, ids: List[int], embeddings: np.array):\n",
    "        \"\"\"\n",
    "        将数据从 Excel 中加载并索引\n",
    "        :param ids: 来自 Excel 的行 ID（可以是某一列唯一标识符）\n",
    "        :param embeddings: 行的嵌入向量\n",
    "        \"\"\"\n",
    "        # 更新 ID 映射\n",
    "        self._update_id_mapping(ids)\n",
    "\n",
    "        # 将 embeddings 转换为 float32 类型\n",
    "        embeddings = embeddings.astype('float32')\n",
    "\n",
    "        # 如果 FAISS 索引尚未训练，则进行训练\n",
    "        if not self.index.is_trained:\n",
    "            self.index.train(embeddings)\n",
    "\n",
    "        # 将 embeddings 添加到 FAISS 索引\n",
    "        self.index.add(embeddings)\n",
    "        print(f'Total data indexed: {len(self.index_id_to_row_id)}')\n",
    "\n",
    "    # 其他方法保持不变\n",
    "\n",
    "    def _update_id_mapping(self, row_ids: List[int]):\n",
    "        \"\"\"更新行 ID 到索引 ID 的映射关系\"\"\"\n",
    "        self.index_id_to_row_id.extend(row_ids)\n",
    "\n",
    "    def search_return_text(self, query: str, top_docs: int, index_batch_size: int = 10000) -> List[Tuple[List[object], List[float]]]:\n",
    "        search_result = self.search_dp(query, top_docs, index_batch_size)\n",
    "        result = []\n",
    "        for i in search_result:\n",
    "            result.append(([self.data_frame['描述'][int(j)] for j in i[0]],i[1]))\n",
    "        return result\n",
    "    def search_dp(self, query: str, top_docs: int, index_batch_size: int = 2048) -> List[Tuple[List[object], List[float]]]:\n",
    "        \"\"\"\n",
    "        执行 dp 查询，返回 Excel 文件行 ID 和相似度得分\n",
    "        :param query_vectors: 查询的嵌入向量\n",
    "        :param top_docs: 返回的最近邻文档数量\n",
    "        :param index_batch_size: 每批次处理的查询数量\n",
    "        :return: 返回每个查询向量对应的最近邻行 ID 和得分\n",
    "        \"\"\"\n",
    "        query_vectors = self.model.encode([query]).astype('float32')\n",
    "        result = []  # 存储所有查询结果\n",
    "\n",
    "        # 计算批次数量\n",
    "        nbatch = (len(query_vectors) - 1) // index_batch_size + 1\n",
    "\n",
    "        # 批量处理查询\n",
    "        for k in tqdm(range(nbatch)):\n",
    "            start_idx = k * index_batch_size\n",
    "            end_idx = min((k + 1) * index_batch_size, len(query_vectors))\n",
    "\n",
    "            q = query_vectors[start_idx: end_idx]\n",
    "\n",
    "            # 使用 FAISS 进行搜索\n",
    "            scores, indexes = self.index.search(q, top_docs)\n",
    "\n",
    "            # 将 FAISS 索引 ID 转换为 Excel 中的行 ID\n",
    "            db_ids = [[str(self.index_id_to_row_id[i]) for i in query_top_idxs] for query_top_idxs in indexes]\n",
    "\n",
    "            # 将每个查询结果添加到最终结果中\n",
    "            result.extend([(db_ids[i], scores[i]) for i in range(len(db_ids))])\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def search_return_text(self, query: str, top_docs: int, index_batch_size: int = 10000) -> List[Tuple[List[object], List[float]]]:\n",
    "        search_result = self.search_dp(query, top_docs, index_batch_size)\n",
    "        result = []\n",
    "        for i in search_result:\n",
    "            result.append(([self.data_frame['描述'][int(j)] for j in i[0]],i[1]))\n",
    "        return result\n",
    "    \n",
    "    \n",
    "retriever = ExcelIndexer(vector_sz=768, model=model, embeddings_file='term_work/embeddings.npy')\n",
    "retriever.load_excel('term_work/更新后的网格文件_提取矛盾类型和严重程度_提取区名.xlsx','描述')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retriever",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
